{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Source: https://github.com/AI4Finance-Foundation/Liquidation-Analysis-using-Multi-Agent-Reinforcement-Learning-ICML-2019/blob/master/Model_training.ipynb'''\n",
    "'''Paper: Multi-agent reinforcement learning for liquidation strategy analysis accepted by ICML 2019 AI in Finance: Applications and Infrastructure for Multi-Agent Learning. (https://arxiv.org/abs/1906.11046)'''\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import finrl_meta.env_execution_optimizing.liquidation.env_execution_optimizing as env\n",
    "from finrl_meta.env_execution_optimizing.liquidation import utils\n",
    "from finrl_meta.env_execution_optimizing.liquidation.ddpg_agent import Agent\n",
    "\n",
    "# Get the default financial and AC Model parameters\n",
    "financial_params, ac_params = utils.get_env_param()\n",
    "\n",
    "# Create simulation environment\n",
    "env = env.MarketEnvironment()\n",
    "\n",
    "# Initialize Feed-forward DNNs for Actor and Critic models.\n",
    "agent1 = Agent(state_size=env.observation_space_dimension(), action_size=env.action_space_dimension(),random_seed = 1225)\n",
    "agent2 = Agent(state_size=env.observation_space_dimension(), action_size=env.action_space_dimension(),random_seed = 108)\n",
    "# Set the liquidation time\n",
    "lqt = 60\n",
    "\n",
    "# Set the number of trades\n",
    "n_trades = 60\n",
    "\n",
    "# Set trader's risk aversion\n",
    "tr1 = 1e-6\n",
    "tr2 = 1e-6\n",
    "\n",
    "# Set the number of episodes to run the simulation\n",
    "episodes = 1300\n",
    "shortfall_list = []\n",
    "shortfall_hist1 = np.array([])\n",
    "shortfall_hist2 = np.array([])\n",
    "shortfall_deque1 = deque(maxlen=100)\n",
    "shortfall_deque2 = deque(maxlen=100)\n",
    "for episode in range(episodes):\n",
    "    # Reset the enviroment\n",
    "    cur_state = env.reset(seed = episode, liquid_time = lqt, num_trades = n_trades, lamb1 = tr1,lamb2 = tr2)\n",
    "\n",
    "    # set the environment to make transactions\n",
    "    env.start_transactions()\n",
    "\n",
    "    for i in range(n_trades + 1):\n",
    "\n",
    "        # Predict the best action for the current state.\n",
    "        cur_state1 = np.delete(cur_state,8)\n",
    "        cur_state2 = np.delete(cur_state,7)\n",
    "        #print(cur_state[5:])\n",
    "        action1 = agent1.act(cur_state1, add_noise = True)\n",
    "        action2 = agent2.act(cur_state2, add_noise = True)\n",
    "        #print(action1,action2)\n",
    "        # Action is performed and new state, reward, info are received.\n",
    "        new_state, reward1, reward2, done1, done2, info = env.step(action1,action2)\n",
    "\n",
    "        # current state, action, reward, new state are stored in the experience replay\n",
    "        new_state1 = np.delete(new_state,8)\n",
    "        new_state2 = np.delete(new_state,7)\n",
    "        agent1.step(cur_state1, action1, reward1, new_state1, done1)\n",
    "        agent2.step(cur_state2, action2, reward2, new_state2, done2)\n",
    "        # roll over new state\n",
    "        cur_state = new_state\n",
    "\n",
    "        if info.done1 and info.done2:\n",
    "            shortfall_hist1 = np.append(shortfall_hist1, info.implementation_shortfall1)\n",
    "            shortfall_deque1.append(info.implementation_shortfall1)\n",
    "\n",
    "            shortfall_hist2 = np.append(shortfall_hist2, info.implementation_shortfall2)\n",
    "            shortfall_deque2.append(info.implementation_shortfall2)\n",
    "            break\n",
    "\n",
    "    if (episode + 1) % 100 == 0: # print average shortfall over last 100 episodes\n",
    "        print('\\rEpisode [{}/{}]\\tAverage Shortfall for Agent1: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque1)))\n",
    "        print('\\rEpisode [{}/{}]\\tAverage Shortfall for Agent2: ${:,.2f}'.format(episode + 1, episodes, np.mean(shortfall_deque2)))\n",
    "        shortfall_list.append([np.mean(shortfall_deque1),np.mean(shortfall_deque2)])\n",
    "print('\\nAverage Implementation Shortfall for Agent1: ${:,.2f} \\n'.format(np.mean(shortfall_hist1)))\n",
    "print('\\nAverage Implementation Shortfall for Agent2: ${:,.2f} \\n'.format(np.mean(shortfall_hist2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}